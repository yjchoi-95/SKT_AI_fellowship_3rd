{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etniX_KTlJ5U"
   },
   "source": [
    "# USAD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N3jM0qLU8MgZ"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "6u1DGKsAlLF-"
   },
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from usad import *\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1crx5rGP9ONf"
   },
   "source": [
    "## EDA - Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfSj4FYL9W8Y"
   },
   "source": [
    "### Normal period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "id": "XeDLxV_r1G9n",
    "outputId": "576538dd-64f2-46fa-8e6f-6c2ffdebad15"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2803, 2)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = \"C:\\\\Users\\\\PC\\\\OneDrive\\\\문서\\\\GitHub\\\\datasets\\\\\"\n",
    "\n",
    "#Read data\n",
    "data = pd.read_csv(data_path + \"Bearing1_1_top5_result.csv\")\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zxFNH5kU9hIE"
   },
   "source": [
    "#### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Mfxj4Uxn9kv4"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "\n",
    "x = data.values\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "scaled_data = pd.DataFrame(x_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "id": "mQ6_U4jn9nlw",
    "outputId": "f1cc1bd6-f1cc-4764-b1cc-2fd989ac4918"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.025027</td>\n",
       "      <td>0.014330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021423</td>\n",
       "      <td>0.015064</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.025027  0.014330\n",
       "1  0.021423  0.015064"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xXJi503b-j_d"
   },
   "source": [
    "### Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vyplttZa-BRN"
   },
   "outputs": [],
   "source": [
    "window_size=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "id": "dzGJMp6Y-BN5",
    "outputId": "2949d278-1313-442c-f06b-275a8c6c6578"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2791, 12, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows_normal=scaled_data.values[np.arange(window_size)[None, :] + np.arange(scaled_data.shape[0]-window_size)[:, None]]\n",
    "windows_normal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k70ZFxGs-_7m"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "N_EPOCHS = 100\n",
    "hidden_size = 10\n",
    "\n",
    "w_size=windows_normal.shape[1]*windows_normal.shape[2]\n",
    "z_size=windows_normal.shape[1]*hidden_size\n",
    "\n",
    "windows_normal_train = windows_normal[:400]\n",
    "windows_normal_test = windows_normal[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import math\n",
    "\n",
    "class Dataloader(Sequence):\n",
    "    def __init__(self, x_set, y_set, batch_size, shuffle=False):\n",
    "        self.x, self.y = x_set, y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle=shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "    # batch 단위로 직접 묶어줘야 함\n",
    "    def __getitem__(self, idx):\n",
    "        # sampler의 역할(index를 batch_size만큼 sampling해줌)\n",
    "        indices = self.indices[idx*self.batch_size:(idx+1)*self.batch_size]\n",
    "\n",
    "        batch_x = [self.x[i] for i in indices]\n",
    "        batch_y = [self.y[i] for i in indices]\n",
    "\n",
    "        return np.array(batch_x), np.array(batch_y)\n",
    "\n",
    "    # epoch이 끝날때마다 실행\n",
    "    def on_epoch_end(self):\n",
    "        self.indices = np.arange(len(self.x))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "windows_normal_train_re = windows_normal_train.reshape(windows_normal_train.shape[0], w_size)\n",
    "windows_normal_test_re = windows_normal_test.reshape(windows_normal_test.shape[0], w_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = Dataloader(windows_normal_train_re,windows_normal_train_re,BATCH_SIZE)\n",
    "test_loader = Dataloader(windows_normal_test_re,windows_normal_test_re,BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* USAD tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model) :\n",
    "    def __init__(self, in_size, latent_size):\n",
    "        super().__init__()\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.Input(shape = (in_size)),\n",
    "            tf.keras.layers.Dense(int(in_size/2), activation = \"relu\"),\n",
    "            tf.keras.layers.Dense(int(in_size/4), activation = \"relu\"),\n",
    "            tf.keras.layers.Dense(latent_size, activation = \"relu\")\n",
    "        ])\n",
    "    \n",
    "    def call(self, x) :\n",
    "        z = self.model(x)\n",
    "        return z\n",
    "    \n",
    "class Decoder(tf.keras.Model) :\n",
    "    def __init__(self, latent_size, out_size):\n",
    "        super().__init__()\n",
    "        self.model = tf.keras.Sequential([\n",
    "            tf.keras.Input(shape = (latent_size)),\n",
    "            tf.keras.layers.Dense(int(out_size/4), activation = \"relu\"),\n",
    "            tf.keras.layers.Dense(int(out_size/2), activation = \"relu\"),\n",
    "            tf.keras.layers.Dense(out_size, activation = \"sigmoid\")\n",
    "        ])\n",
    "    \n",
    "    def call(self, x) :\n",
    "        w = self.model(x)\n",
    "        return w\n",
    "    \n",
    "class UsadModel(tf.keras.Model):\n",
    "    def __init__(self, w_size, z_size, alpha = .5, beta = .5):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(w_size, z_size)\n",
    "        self.decoder1 = Decoder(z_size, w_size)\n",
    "        self.decoder2 = Decoder(z_size, w_size)\n",
    "        \n",
    "        self.latent_vector = self.encoder(self.encoder.model.inputs)\n",
    "        self.ae_output1 = self.decoder1(self.latent_vector)\n",
    "        self.ae_output2 = self.decoder2(self.latent_vector)\n",
    "\n",
    "        self.ae_model1 = tf.keras.Model(inputs = self.encoder.model.inputs, outputs = self.ae_output1)\n",
    "        self.ae_model2 = tf.keras.Model(inputs = self.encoder.model.inputs, outputs = self.ae_output2)\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        self.alpha, self.beta = alpha, beta\n",
    "        \n",
    "    def evaluate(self, val_loader, n):\n",
    "        outputs = [self.validation_step(batch, n) for batch in val_loader]\n",
    "        return self.validation_epoch_end(outputs)\n",
    "    \n",
    "    def testing(self, test_loader):\n",
    "        results=[]\n",
    "        for batch, _ in test_loader:\n",
    "            w1=self.ae_model1(batch)\n",
    "            w2=self.ae_model2(w1)\n",
    "            results.append(self.alpha*np.mean((batch-w1).numpy()**2, axis = 1)+self.beta*np.mean((batch-w2).numpy()**2, axis = 1))\n",
    "        return results\n",
    "        \n",
    "    def call(self, x):\n",
    "        z = self.encoder(x)\n",
    "        w1 = self.decoder1(z)\n",
    "        w2 = self.decoder2(z)\n",
    "        w3 = self.decoder2(self.encoder(w1))\n",
    "        return w1, w2, w3\n",
    "    \n",
    "    def loss_fn(self, batch, n) :\n",
    "        loss1 = 1/n*tf.reduce_mean(tf.square(batch-self.w1)) + (1-1/n)*tf.reduce_mean(tf.square(batch-self.w3))\n",
    "        loss2 = 1/n*tf.reduce_mean(tf.square(batch-self.w2)) + (1-1/n)*tf.reduce_mean(tf.square(batch-self.w3))\n",
    "        \n",
    "        return loss1, loss2\n",
    "    \n",
    "    def training(self, train_loader, val_loader, num_epochs):\n",
    "        for n in range(num_epochs): \n",
    "            n += 1\n",
    "            \n",
    "            loss1_list, loss2_list = [], []\n",
    "            self.history = []\n",
    "            \n",
    "            # Iterate over the batches of a dataset.\n",
    "            for x_batch_train, y_batch_train in train_loader:\n",
    "                with tf.GradientTape() as ae1_tape, tf.GradientTape() as ae2_tape:\n",
    "                    self.z = self.encoder(x_batch_train)\n",
    "                    self.w1 = self.decoder1(self.z)\n",
    "                    self.w2 = self.decoder2(self.z)\n",
    "                    self.w3 = self.decoder2(self.encoder(self.w1))\n",
    "\n",
    "                    # Loss value for this minibatch\n",
    "                    loss1, loss2 = self.loss_fn(x_batch_train, n)\n",
    "                    \n",
    "                    # Add extra losses created during this forward pass:\n",
    "                    #loss_value += sum(model.losses)\n",
    "\n",
    "                grads_ae1 = ae1_tape.gradient(loss1, self.ae_model1.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(grads_ae1, self.ae_model1.trainable_weights))\n",
    "                grads_ae2 = ae2_tape.gradient(loss2, self.ae_model2.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(grads_ae2, self.ae_model2.trainable_weights))\n",
    "                loss1_list.append(loss1)\n",
    "                loss2_list.append(loss2)\n",
    "                \n",
    "            #print(\"Epoch [{}], train_loss1: {:.4f}, train_loss2: {:.4f}\".format(n, np.mean(loss1_list), np.mean(loss2_list)))\n",
    "            result = self.evaluate(val_loader, n)\n",
    "            self.epoch_end(n, result)\n",
    "            self.history.append(result)\n",
    "            \n",
    "    def validation_step(self, batch, n):\n",
    "        z = self.encoder(batch)\n",
    "        w1 = self.decoder1(z)\n",
    "        w2 = self.decoder2(z)\n",
    "        w3 = self.decoder2(self.encoder(w1))\n",
    "        loss1 = 1/n*tf.reduce_mean(tf.square(batch-w1)) + (1-1/n)*tf.reduce_mean(tf.square(batch-w3))\n",
    "        loss2 = 1/n*tf.reduce_mean(tf.square(batch-w2)) + (1-1/n)*tf.reduce_mean(tf.square(batch-w3))\n",
    "        \n",
    "        return {'val_loss1': loss1, 'val_loss2': loss2}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses1 = [x['val_loss1'] for x in outputs]\n",
    "        epoch_loss1 = tf.reduce_mean(batch_losses1)\n",
    "        batch_losses2 = [x['val_loss2'] for x in outputs]\n",
    "        epoch_loss2 = tf.reduce_mean(batch_losses2)\n",
    "        return {'val_loss1': epoch_loss1.numpy(), 'val_loss2': epoch_loss2.numpy()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss1: {:.4f}, val_loss2: {:.4f}\".format(epoch, result['val_loss1'], result['val_loss2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Encoder.call of <__main__.Encoder object at 0x000001B05D56D888>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Encoder.call of <__main__.Encoder object at 0x000001B05D56D888>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x000001B05D676188>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x000001B05D676188>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x000001B05D669BC8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x000001B05D669BC8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "Epoch [1], val_loss1: 0.2132, val_loss2: 0.2134\n",
      "Epoch [2], val_loss1: 0.2076, val_loss2: 0.2071\n",
      "Epoch [3], val_loss1: 0.1920, val_loss2: 0.1902\n",
      "Epoch [4], val_loss1: 0.1593, val_loss2: 0.1564\n",
      "Epoch [5], val_loss1: 0.1214, val_loss2: 0.1202\n",
      "Epoch [6], val_loss1: 0.0780, val_loss2: 0.0792\n",
      "Epoch [7], val_loss1: 0.0412, val_loss2: 0.0431\n",
      "Epoch [8], val_loss1: 0.0195, val_loss2: 0.0207\n",
      "Epoch [9], val_loss1: 0.0084, val_loss2: 0.0087\n",
      "Epoch [10], val_loss1: 0.0057, val_loss2: 0.0055\n",
      "Epoch [11], val_loss1: 0.0054, val_loss2: 0.0053\n",
      "Epoch [12], val_loss1: 0.0054, val_loss2: 0.0054\n",
      "Epoch [13], val_loss1: 0.0054, val_loss2: 0.0054\n",
      "Epoch [14], val_loss1: 0.0055, val_loss2: 0.0055\n",
      "Epoch [15], val_loss1: 0.0055, val_loss2: 0.0055\n",
      "Epoch [16], val_loss1: 0.0056, val_loss2: 0.0056\n",
      "Epoch [17], val_loss1: 0.0056, val_loss2: 0.0056\n",
      "Epoch [18], val_loss1: 0.0057, val_loss2: 0.0057\n",
      "Epoch [19], val_loss1: 0.0057, val_loss2: 0.0057\n",
      "Epoch [20], val_loss1: 0.0057, val_loss2: 0.0057\n",
      "Epoch [21], val_loss1: 0.0057, val_loss2: 0.0057\n",
      "Epoch [22], val_loss1: 0.0057, val_loss2: 0.0057\n",
      "Epoch [23], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [24], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [25], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [26], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [27], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [28], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [29], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [30], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [31], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [32], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [33], val_loss1: 0.0058, val_loss2: 0.0059\n",
      "Epoch [34], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [35], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [36], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [37], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [38], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [39], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [40], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [41], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [42], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [43], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [44], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [45], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [46], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [47], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [48], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [49], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [50], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [51], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [52], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [53], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [54], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [55], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [56], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [57], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [58], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [59], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [60], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [61], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [62], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [63], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [64], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [65], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [66], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [67], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [68], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [69], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [70], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [71], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [72], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [73], val_loss1: 0.0059, val_loss2: 0.0059\n",
      "Epoch [74], val_loss1: 0.0058, val_loss2: 0.0059\n",
      "Epoch [75], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [76], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [77], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [78], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [79], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [80], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [81], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [82], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [83], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [84], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [85], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [86], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [87], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [88], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [89], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [90], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [91], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [92], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [93], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [94], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [95], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [96], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [97], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [98], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [99], val_loss1: 0.0058, val_loss2: 0.0058\n",
      "Epoch [100], val_loss1: 0.0058, val_loss2: 0.0058\n"
     ]
    }
   ],
   "source": [
    "model_usad = UsadModel(w_size, z_size)\n",
    "model_usad.training(train_loader, test_loader, N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_usad.testing(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.concatenate([np.stack(results[:-1]).flatten(), results[-1].flatten()])\n",
    "#y_test = np.concatenate([np.zeros(windows_normal_test.shape[0]),np.ones(windows_attack.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1b05db14708>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(y_pred).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly gap maximize net\n",
    "\n",
    "* 제안 아이디어 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UsadModel_AGM(tf.keras.Model):\n",
    "    def __init__(self, w_size, z_size, alpha = .5, beta = .5):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(w_size, z_size)\n",
    "        self.decoder1 = Decoder(z_size, w_size)\n",
    "        self.decoder2 = Decoder(z_size, w_size)\n",
    "        \n",
    "        self.latent_vector = self.encoder(self.encoder.model.inputs)\n",
    "        self.ae_output1 = self.decoder1(self.latent_vector)\n",
    "        self.ae_output2 = self.decoder2(self.latent_vector)\n",
    "\n",
    "        self.ae_model1 = tf.keras.Model(inputs = self.encoder.model.inputs, outputs = self.ae_output1)\n",
    "        self.ae_model2 = tf.keras.Model(inputs = self.encoder.model.inputs, outputs = self.ae_output2)\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        self.alpha, self.beta = alpha, beta\n",
    "        \n",
    "    def evaluate(self, val_loader, n):\n",
    "        outputs = [self.validation_step(batch, n) for batch in val_loader]\n",
    "        return self.validation_epoch_end(outputs)\n",
    "\n",
    "    def testing(self, test_loader):\n",
    "        results=[]\n",
    "        for batch, _ in test_loader:\n",
    "            w1=self.ae_model1(batch)\n",
    "            w2=self.ae_model2(w1)\n",
    "            results.append(self.alpha*np.mean((batch-w1).numpy()**2, axis = 1)+self.beta*np.mean((batch-w2).numpy()**2, axis = 1))\n",
    "        return results\n",
    "        \n",
    "    def call(self, x):\n",
    "        z = self.encoder(x)\n",
    "        w1 = self.decoder1(z)\n",
    "        w2 = self.decoder2(z)\n",
    "        w3 = self.decoder2(self.encoder(w1))\n",
    "        return w1, w2, w3\n",
    "    \n",
    "    def loss_fn(self, batch, n) :\n",
    "        loss1 = 1/n*tf.reduce_mean(tf.square(batch-self.w1)) + (1-1/n)*tf.reduce_mean(tf.square(batch-self.w3))\n",
    "        loss2 = 1/n*tf.reduce_mean(tf.square(batch-self.w2)) + (1-1/n)*tf.reduce_mean(tf.square(batch-self.w3))\n",
    "        \n",
    "        return loss1, loss2\n",
    "    \n",
    "    def training(self, train_loader, val_loader, num_epochs):\n",
    "        self.history = []\n",
    "        \n",
    "        for n in range(num_epochs): \n",
    "            n += 1\n",
    "            \n",
    "            loss1_list, loss2_list, loss3_list = [], [], []\n",
    "            \n",
    "            # Iterate over the batches of a dataset.\n",
    "            for x_batch_train, y_batch_train in train_loader:\n",
    "                with tf.GradientTape() as ae1_tape, tf.GradientTape() as ae2_tape, tf.GradientTape() as pg_tape:\n",
    "                    self.z = self.encoder(x_batch_train)\n",
    "                    self.w1 = self.decoder1(self.z)\n",
    "                    self.w2 = self.decoder2(self.z)\n",
    "                    self.w3 = self.decoder2(self.encoder(self.w1))\n",
    "                    \n",
    "                    real_recon1 = x_batch_train-self.w1\n",
    "                    real_recon2 = x_batch_train-self.w2\n",
    "                    fake_recon = x_batch_train-self.w3\n",
    "\n",
    "                    # Loss value for this minibatch\n",
    "                    loss1, loss2 = self.loss_fn(x_batch_train, n)\n",
    "                    \n",
    "                    pg_advantage = tf.stop_gradient(tf.reduce_mean(tf.square(fake_recon))-tf.reduce_mean(tf.square(real_recon2)))\n",
    "                    loss3 = -tf.reduce_mean(tf.math.log(self.w3+1e-6) * pg_advantage)\n",
    "        \n",
    "                    \n",
    "                    # Add extra losses created during this forward pass:\n",
    "                    #loss_value += sum(model.losses)\n",
    "\n",
    "                grads_ae1 = ae1_tape.gradient(loss1, self.ae_model1.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(grads_ae1, self.ae_model1.trainable_weights))\n",
    "                grads_ae2 = ae2_tape.gradient(loss2, self.ae_model2.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(grads_ae2, self.ae_model2.trainable_weights))\n",
    "                grads_ae3 = pg_tape.gradient(loss3, self.ae_model2.trainable_weights)\n",
    "                self.optimizer.apply_gradients(zip(grads_ae3, self.ae_model2.trainable_weights))\n",
    "                \n",
    "                loss1_list.append(loss1)\n",
    "                loss2_list.append(loss2)\n",
    "                loss3_list.append(loss3)\n",
    "                \n",
    "            result = self.evaluate(val_loader, n)\n",
    "            self.epoch_end(n, result)\n",
    "            self.history.append(result)\n",
    "                \n",
    "            #print(\"Epoch [{}], train_loss1: {:.4f}, train_loss2: {:.4f}, train_loss3: {:.4f}\".format(n, np.mean(loss1_list), np.mean(loss2_list), np.mean(loss3_list)))\n",
    "            \n",
    "    def validation_step(self, batch, n):\n",
    "        z = self.encoder(batch)\n",
    "        w1 = self.decoder1(z)\n",
    "        w2 = self.decoder2(z)\n",
    "        w3 = self.decoder2(self.encoder(w1))\n",
    "        \n",
    "        real_recon1 = batch-w1\n",
    "        real_recon2 = batch-w2\n",
    "        fake_recon = batch-w3\n",
    "                \n",
    "        loss1 = 1/n*tf.reduce_mean(tf.square(batch-w1)) + (1-1/n)*tf.reduce_mean(tf.square(batch-w3))\n",
    "        loss2 = 1/n*tf.reduce_mean(tf.square(batch-w2)) + (1-1/n)*tf.reduce_mean(tf.square(batch-w3))\n",
    "        \n",
    "        pg_advantage = tf.reduce_mean(tf.square(fake_recon))-tf.reduce_mean(tf.square(real_recon2))\n",
    "        loss3 = -tf.reduce_mean(tf.math.log(w3+1e-6) * pg_advantage)\n",
    "        \n",
    "        return {'val_loss1': loss1, 'val_loss2': loss2, 'val_loss3': loss3}\n",
    "    \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses1 = [x['val_loss1'] for x in outputs]\n",
    "        epoch_loss1 = tf.reduce_mean(batch_losses1)\n",
    "        batch_losses2 = [x['val_loss2'] for x in outputs]\n",
    "        epoch_loss2 = tf.reduce_mean(batch_losses2)\n",
    "        batch_losses3 = [x['val_loss3'] for x in outputs]\n",
    "        epoch_loss3 = tf.reduce_mean(batch_losses3)\n",
    "        return {'val_loss1': epoch_loss1.numpy(), 'val_loss2': epoch_loss2.numpy(), 'val_loss3': epoch_loss3.numpy()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], val_loss1: {:.4f}, val_loss2: {:.4f}, val_loss2: {:.4f}\".format(epoch, result['val_loss1'], result['val_loss2'], result['val_loss3']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Encoder.call of <__main__.Encoder object at 0x000001B0642F4D08>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Encoder.call of <__main__.Encoder object at 0x000001B0642F4D08>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x000001B0642F44C8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x000001B0642F44C8>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Decoder.call of <__main__.Decoder object at 0x000001B0642D4888>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Decoder.call of <__main__.Decoder object at 0x000001B0642D4888>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "usad_agm = UsadModel_AGM(w_size,z_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], val_loss1: 0.2063, val_loss2: 0.2080, val_loss2: -0.0068\n",
      "Epoch [2], val_loss1: 0.1681, val_loss2: 0.1669, val_loss2: -0.0303\n",
      "Epoch [3], val_loss1: 0.1154, val_loss2: 0.1174, val_loss2: -0.0316\n",
      "Epoch [4], val_loss1: 0.0613, val_loss2: 0.0685, val_loss2: -0.0410\n",
      "Epoch [5], val_loss1: 0.0339, val_loss2: 0.0393, val_loss2: -0.0280\n",
      "Epoch [6], val_loss1: 0.0178, val_loss2: 0.0203, val_loss2: -0.0255\n",
      "Epoch [7], val_loss1: 0.0085, val_loss2: 0.0092, val_loss2: -0.0311\n",
      "Epoch [8], val_loss1: 0.0063, val_loss2: 0.0062, val_loss2: -0.0376\n",
      "Epoch [9], val_loss1: 0.0062, val_loss2: 0.0058, val_loss2: -0.0366\n",
      "Epoch [10], val_loss1: 0.0062, val_loss2: 0.0057, val_loss2: -0.0337\n",
      "Epoch [11], val_loss1: 0.0062, val_loss2: 0.0057, val_loss2: -0.0322\n",
      "Epoch [12], val_loss1: 0.0060, val_loss2: 0.0056, val_loss2: -0.0289\n",
      "Epoch [13], val_loss1: 0.0058, val_loss2: 0.0053, val_loss2: -0.0252\n",
      "Epoch [14], val_loss1: 0.0055, val_loss2: 0.0051, val_loss2: -0.0208\n",
      "Epoch [15], val_loss1: 0.0053, val_loss2: 0.0050, val_loss2: -0.0170\n",
      "Epoch [16], val_loss1: 0.0052, val_loss2: 0.0049, val_loss2: -0.0147\n",
      "Epoch [17], val_loss1: 0.0051, val_loss2: 0.0049, val_loss2: -0.0133\n",
      "Epoch [18], val_loss1: 0.0050, val_loss2: 0.0048, val_loss2: -0.0123\n",
      "Epoch [19], val_loss1: 0.0050, val_loss2: 0.0048, val_loss2: -0.0116\n",
      "Epoch [20], val_loss1: 0.0049, val_loss2: 0.0048, val_loss2: -0.0110\n",
      "Epoch [21], val_loss1: 0.0049, val_loss2: 0.0048, val_loss2: -0.0106\n",
      "Epoch [22], val_loss1: 0.0049, val_loss2: 0.0048, val_loss2: -0.0101\n",
      "Epoch [23], val_loss1: 0.0049, val_loss2: 0.0048, val_loss2: -0.0097\n",
      "Epoch [24], val_loss1: 0.0049, val_loss2: 0.0048, val_loss2: -0.0094\n",
      "Epoch [25], val_loss1: 0.0048, val_loss2: 0.0048, val_loss2: -0.0090\n",
      "Epoch [26], val_loss1: 0.0048, val_loss2: 0.0048, val_loss2: -0.0087\n",
      "Epoch [27], val_loss1: 0.0048, val_loss2: 0.0048, val_loss2: -0.0083\n",
      "Epoch [28], val_loss1: 0.0048, val_loss2: 0.0048, val_loss2: -0.0079\n",
      "Epoch [29], val_loss1: 0.0048, val_loss2: 0.0048, val_loss2: -0.0074\n",
      "Epoch [30], val_loss1: 0.0048, val_loss2: 0.0048, val_loss2: -0.0067\n",
      "Epoch [31], val_loss1: 0.0049, val_loss2: 0.0049, val_loss2: -0.0061\n",
      "Epoch [32], val_loss1: 0.0050, val_loss2: 0.0050, val_loss2: -0.0055\n",
      "Epoch [33], val_loss1: 0.0051, val_loss2: 0.0051, val_loss2: -0.0051\n",
      "Epoch [34], val_loss1: 0.0052, val_loss2: 0.0052, val_loss2: -0.0048\n",
      "Epoch [35], val_loss1: 0.0052, val_loss2: 0.0052, val_loss2: -0.0046\n",
      "Epoch [36], val_loss1: 0.0053, val_loss2: 0.0053, val_loss2: -0.0044\n",
      "Epoch [37], val_loss1: 0.0053, val_loss2: 0.0053, val_loss2: -0.0043\n",
      "Epoch [38], val_loss1: 0.0053, val_loss2: 0.0053, val_loss2: -0.0043\n",
      "Epoch [39], val_loss1: 0.0054, val_loss2: 0.0054, val_loss2: -0.0042\n",
      "Epoch [40], val_loss1: 0.0054, val_loss2: 0.0054, val_loss2: -0.0041\n",
      "Epoch [41], val_loss1: 0.0054, val_loss2: 0.0054, val_loss2: -0.0040\n",
      "Epoch [42], val_loss1: 0.0054, val_loss2: 0.0054, val_loss2: -0.0040\n",
      "Epoch [43], val_loss1: 0.0055, val_loss2: 0.0055, val_loss2: -0.0039\n",
      "Epoch [44], val_loss1: 0.0055, val_loss2: 0.0055, val_loss2: -0.0038\n",
      "Epoch [45], val_loss1: 0.0055, val_loss2: 0.0055, val_loss2: -0.0038\n",
      "Epoch [46], val_loss1: 0.0055, val_loss2: 0.0055, val_loss2: -0.0037\n",
      "Epoch [47], val_loss1: 0.0056, val_loss2: 0.0056, val_loss2: -0.0036\n",
      "Epoch [48], val_loss1: 0.0056, val_loss2: 0.0056, val_loss2: -0.0036\n",
      "Epoch [49], val_loss1: 0.0056, val_loss2: 0.0056, val_loss2: -0.0035\n",
      "Epoch [50], val_loss1: 0.0056, val_loss2: 0.0056, val_loss2: -0.0034\n",
      "Epoch [51], val_loss1: 0.0057, val_loss2: 0.0057, val_loss2: -0.0033\n",
      "Epoch [52], val_loss1: 0.0057, val_loss2: 0.0057, val_loss2: -0.0033\n",
      "Epoch [53], val_loss1: 0.0057, val_loss2: 0.0057, val_loss2: -0.0032\n",
      "Epoch [54], val_loss1: 0.0057, val_loss2: 0.0057, val_loss2: -0.0031\n",
      "Epoch [55], val_loss1: 0.0057, val_loss2: 0.0057, val_loss2: -0.0031\n",
      "Epoch [56], val_loss1: 0.0057, val_loss2: 0.0057, val_loss2: -0.0030\n",
      "Epoch [57], val_loss1: 0.0058, val_loss2: 0.0058, val_loss2: -0.0030\n",
      "Epoch [58], val_loss1: 0.0058, val_loss2: 0.0058, val_loss2: -0.0029\n",
      "Epoch [59], val_loss1: 0.0058, val_loss2: 0.0058, val_loss2: -0.0029\n",
      "Epoch [60], val_loss1: 0.0058, val_loss2: 0.0058, val_loss2: -0.0028\n",
      "Epoch [61], val_loss1: 0.0058, val_loss2: 0.0058, val_loss2: -0.0028\n",
      "Epoch [62], val_loss1: 0.0058, val_loss2: 0.0058, val_loss2: -0.0028\n",
      "Epoch [63], val_loss1: 0.0058, val_loss2: 0.0058, val_loss2: -0.0027\n",
      "Epoch [64], val_loss1: 0.0058, val_loss2: 0.0058, val_loss2: -0.0027\n",
      "Epoch [65], val_loss1: 0.0059, val_loss2: 0.0059, val_loss2: -0.0027\n",
      "Epoch [66], val_loss1: 0.0059, val_loss2: 0.0059, val_loss2: -0.0026\n",
      "Epoch [67], val_loss1: 0.0059, val_loss2: 0.0059, val_loss2: -0.0026\n",
      "Epoch [68], val_loss1: 0.0059, val_loss2: 0.0059, val_loss2: -0.0025\n",
      "Epoch [69], val_loss1: 0.0059, val_loss2: 0.0059, val_loss2: -0.0025\n",
      "Epoch [70], val_loss1: 0.0059, val_loss2: 0.0059, val_loss2: -0.0025\n",
      "Epoch [71], val_loss1: 0.0059, val_loss2: 0.0059, val_loss2: -0.0024\n",
      "Epoch [72], val_loss1: 0.0059, val_loss2: 0.0059, val_loss2: -0.0024\n",
      "Epoch [73], val_loss1: 0.0059, val_loss2: 0.0059, val_loss2: -0.0023\n",
      "Epoch [74], val_loss1: 0.0060, val_loss2: 0.0060, val_loss2: -0.0023\n",
      "Epoch [75], val_loss1: 0.0060, val_loss2: 0.0060, val_loss2: -0.0023\n",
      "Epoch [76], val_loss1: 0.0060, val_loss2: 0.0060, val_loss2: -0.0022\n",
      "Epoch [77], val_loss1: 0.0060, val_loss2: 0.0060, val_loss2: -0.0022\n",
      "Epoch [78], val_loss1: 0.0060, val_loss2: 0.0060, val_loss2: -0.0022\n",
      "Epoch [79], val_loss1: 0.0060, val_loss2: 0.0060, val_loss2: -0.0021\n",
      "Epoch [80], val_loss1: 0.0060, val_loss2: 0.0060, val_loss2: -0.0021\n",
      "Epoch [81], val_loss1: 0.0060, val_loss2: 0.0060, val_loss2: -0.0020\n",
      "Epoch [82], val_loss1: 0.0060, val_loss2: 0.0060, val_loss2: -0.0020\n",
      "Epoch [83], val_loss1: 0.0060, val_loss2: 0.0060, val_loss2: -0.0020\n",
      "Epoch [84], val_loss1: 0.0061, val_loss2: 0.0061, val_loss2: -0.0019\n",
      "Epoch [85], val_loss1: 0.0061, val_loss2: 0.0061, val_loss2: -0.0019\n",
      "Epoch [86], val_loss1: 0.0061, val_loss2: 0.0061, val_loss2: -0.0018\n",
      "Epoch [87], val_loss1: 0.0061, val_loss2: 0.0061, val_loss2: -0.0018\n",
      "Epoch [88], val_loss1: 0.0061, val_loss2: 0.0061, val_loss2: -0.0017\n",
      "Epoch [89], val_loss1: 0.0061, val_loss2: 0.0061, val_loss2: -0.0017\n",
      "Epoch [90], val_loss1: 0.0061, val_loss2: 0.0061, val_loss2: -0.0016\n",
      "Epoch [91], val_loss1: 0.0062, val_loss2: 0.0062, val_loss2: -0.0016\n",
      "Epoch [92], val_loss1: 0.0062, val_loss2: 0.0062, val_loss2: -0.0016\n",
      "Epoch [93], val_loss1: 0.0062, val_loss2: 0.0062, val_loss2: -0.0016\n",
      "Epoch [94], val_loss1: 0.0062, val_loss2: 0.0062, val_loss2: -0.0016\n",
      "Epoch [95], val_loss1: 0.0062, val_loss2: 0.0062, val_loss2: -0.0016\n",
      "Epoch [96], val_loss1: 0.0062, val_loss2: 0.0062, val_loss2: -0.0016\n",
      "Epoch [97], val_loss1: 0.0062, val_loss2: 0.0062, val_loss2: -0.0016\n",
      "Epoch [98], val_loss1: 0.0062, val_loss2: 0.0062, val_loss2: -0.0016\n",
      "Epoch [99], val_loss1: 0.0062, val_loss2: 0.0062, val_loss2: -0.0016\n",
      "Epoch [100], val_loss1: 0.0062, val_loss2: 0.0062, val_loss2: -0.0016\n"
     ]
    }
   ],
   "source": [
    "usad_agm.training(train_loader, test_loader, N_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ymhjbmvR_DgJ"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model_usad.testing(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.concatenate([np.stack(results[:-1]).flatten(), results[-1].flatten()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1b063c67708>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pd.DataFrame(y_pred).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "USAD_test.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
